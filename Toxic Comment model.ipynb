{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffae462e-22f4-4330-b2b9-08d2269b5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Fitting tokenizer...\n",
      "Training data shape: (143613, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m64,128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,072,774</span> (7.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,072,774\u001b[0m (7.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,072,774</span> (7.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,072,774\u001b[0m (7.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/4\n",
      "\u001b[1m2244/2244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 32ms/step - accuracy: 0.9322 - loss: 0.0663 - val_accuracy: 0.9940 - val_loss: 0.0487\n",
      "Epoch 2/4\n",
      "\u001b[1m2244/2244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 36ms/step - accuracy: 0.9847 - loss: 0.0456 - val_accuracy: 0.9940 - val_loss: 0.0466\n",
      "Epoch 3/4\n",
      "\u001b[1m2244/2244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9660 - loss: 0.0376 - val_accuracy: 0.9940 - val_loss: 0.0481\n",
      "Epoch 4/4\n",
      "\u001b[1m2244/2244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 37ms/step - accuracy: 0.8777 - loss: 0.0304 - val_accuracy: 0.9454 - val_loss: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n",
      "Evaluating...\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "toxic: F1 = 0.7681\n",
      "severe_toxic: F1 = 0.1358\n",
      "obscene: F1 = 0.8016\n",
      "threat: F1 = 0.1905\n",
      "insult: F1 = 0.7327\n",
      "identity_hate: F1 = 0.0513\n",
      "Macro F1: 0.44667146427849175\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# simple_train.py\n",
    "\"\"\"\n",
    "Simplified TextCNN training script for Jigsaw Toxic Comment dataset.\n",
    "- No main() function\n",
    "- Clean, minimal, beginner-friendly\n",
    "- NOW includes inline text cleaning (no functions)\n",
    "- Saves: model.h5 and tokenizer.pkl\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_PATH = r\"C:\\Users\\AGASTYA\\Downloads\\ToxicCommentApp\\train.csv\"\n",
    "MODEL_PATH = \"model.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "MAX_LEN = 150\n",
    "EMBED_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "LABELS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# INLINE CLEANING (NO FUNCTIONS — you requested this)\n",
    "# ------------------------------------------------------\n",
    "df[\"comment_text\"] = df[\"comment_text\"].fillna(\"empty\").astype(str)\n",
    "\n",
    "# lowercase\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.lower()\n",
    "\n",
    "# remove URLs\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(r\"http\\S+|www\\.\\S+\", \" \", regex=True)\n",
    "\n",
    "# replace @mentions and hashtags\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(r\"@\\w+\", \" @user \", regex=True)\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(r\"#\\w+\", \" #tag \", regex=True)\n",
    "\n",
    "# expand simple contractions\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"can't\", \"cannot\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"won't\", \"will not\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"n't\", \" not\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"'re\", \" are\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"'s\", \" is\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"'d\", \" would\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"'ll\", \" will\")\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(\"'ve\", \" have\")\n",
    "\n",
    "# remove punctuation except ! ? .\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(r\"[^a-z0-9\\s\\!\\?\\.]\", \" \", regex=True)\n",
    "\n",
    "# reduce multiple spaces\n",
    "df[\"comment_text\"] = df[\"comment_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Ensure all labels exist\n",
    "for col in LABELS:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "\n",
    "texts = df[\"comment_text\"].values\n",
    "labels = df[LABELS].values.astype(\"float32\")\n",
    "\n",
    "# Train/validation split\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    texts, labels, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Fitting tokenizer...\")\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert text → sequences → padded arrays\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=MAX_LEN)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_texts), maxlen=MAX_LEN)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "\n",
    "# ----- Build TextCNN Model -----\n",
    "def build_textcnn():\n",
    "    inp = layers.Input(shape=(MAX_LEN,))\n",
    "    x = layers.Embedding(NUM_WORDS, EMBED_DIM)(inp)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(len(LABELS), activation=\"sigmoid\")(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "model = build_textcnn()\n",
    "model.compile(optimizer=optimizers.Adam(1e-3),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Save model + tokenizer\n",
    "print(\"Saving model and tokenizer...\")\n",
    "model.save(MODEL_PATH)\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Evaluate with F1-score\n",
    "print(\"Evaluating...\")\n",
    "y_pred_prob = model.predict(X_val)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "for i, col in enumerate(LABELS):\n",
    "    f1 = f1_score(y_val[:, i], y_pred[:, i], zero_division=0)\n",
    "    print(f\"{col}: F1 = {f1:.4f}\")\n",
    "\n",
    "print(\"Macro F1:\",\n",
    "      np.mean([f1_score(y_val[:, i], y_pred[:, i], zero_division=0)\n",
    "               for i in range(len(LABELS))]))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4dddc-7228-4e7e-8d28-d280728b72f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
